{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d2388f-6c62-48b2-9b58-dcb9a491fb28",
   "metadata": {},
   "source": [
    "## StreetFighter Setup (Default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00027490-7065-49df-aba3-e6a7d96e736e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install gym-retro --use-pep517\n",
    "\n",
    "#python version = 3.7.9\n",
    "# libraries used: \n",
    "# gym = 0.19\n",
    "# openai-gym-retro = 0.8.0\n",
    "# pytorch\n",
    "# optuna\n",
    "# stable_baseline3 = 1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3309e4-9b64-483a-97e6-f03ecf437dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import retro to be able to install streetfighter\n",
    "import retro\n",
    "\n",
    "import time # to slow down game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b8650-299d-44a2-aeb1-4b9a18bad189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see different retro games\n",
    "retro.data.list_games()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6038d3f-be21-4f93-9e75-f508201a64d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# python -m retro.import . # needs to be run from the roms folder to be able to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54198095-fc40-4151-afd2-cc32f4cc9580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# starts up the game environment\n",
    "env = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis') # IT FINALLY WORKED. THIS IS PROGRESS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e9e2e-44bd-4c5b-a436-ae2eeb5b7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() # run this to close a previous instance of the game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a099528c-6fa7-4247-b157-ad67f4b5d68e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# actions, moves or decisions available for the character or agent\n",
    "env.action_space.sample() # look into what the floop this means!, along with observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c2917-76fe-41b2-9bb3-fa14cc9e1eee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what the character or agent can view about the current situation \n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6686d-32b8-412f-8307-4258e56f2a3d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs = env.reset() # resets game to starting state\n",
    "done = False # tells if game is dead\n",
    "\n",
    "for game in range(1): # loops through game one time\n",
    "    while not done: # keep looping until dead\n",
    "        if done: # if dead, start(reset) again. \n",
    "            obs = env.reset()\n",
    "        env.render() # renders environement\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "        time.sleep(0.01) # to see some of the actions better\n",
    "        print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d48f12-32da-4792-8524-676762ce52fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7402b2-24c6-47dc-818b-72fb46ef3b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56d5adaa-ffec-4f88-9245-5cd4b918ddea",
   "metadata": {},
   "source": [
    "## Custom Environment Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f6c3a5-174e-4339-97c8-da95b1812724",
   "metadata": {},
   "source": [
    " ### What needs to be done:\n",
    "- observation preprocess - grayscale, from delta, rezize the gram so we have less pixels\n",
    "\n",
    "- filter the action - parameter\n",
    "- Reward function - set to score for now\n",
    "\n",
    "\n",
    "\n",
    "- Grayscale:\n",
    "\n",
    "Change the game's view to black and white.\n",
    "Why it's necessary: Colors often have lots of extra information that may not be needed. By using grayscale, you reduce the data's complexity, making it faster and sometimes easier for the agent to process and learn from.\n",
    "\n",
    "- From Delta:\n",
    "\n",
    "Look at how things change from one moment to the next, rather than the entire scene.\n",
    "Why it's necessary: Often, the change in the environment (like movement of objects) is more important than the entire scene. By focusing on changes, the agent can better understand and react to dynamic elements, like moving obstacles.\n",
    "\n",
    "- Resize (to have less pixels):\n",
    "\n",
    "Make the game's view smaller by reducing the number of pixels.\n",
    "Why it's necessary: A smaller image is faster to process. By reducing the image's size, you make computations quicker and require less memory, making the learning process more efficient.\n",
    "\n",
    "\n",
    "\n",
    "In essence, the StreetFighter class is a custom setup for an AI to play Street Fighter. It ensures the AI sees the game in a simpler way, gets feedback after every move, and learns from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659328a-da41-4a50-8320-030a1b08e110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f1ab50-3a6e-4325-bfe2-1a41a9e52ad3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b5224-ba41-4803-9103-a5502ab2ddfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import environment base class for a wrapper\n",
    "from gym import Env\n",
    "# import the space shapes for the environment\n",
    "from gym.spaces import MultiBinary, Box\n",
    "# import numpy to calculate frame delta\n",
    "import numpy as np\n",
    "# import opencv for grayscaling\n",
    "import cv2 \n",
    "\n",
    "#import matplotlib for plotting the image\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9fdfc-d0f5-453b-8c73-b73432b16080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create custom training environment \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class StreetFighter(Env):\n",
    "    def __init__(self):\n",
    "        \n",
    "#         When you turn on the console (__init__ method), it sets\n",
    "#         up the controls (actions the AI can take) and the screen \n",
    "#         size (what the AI sees). It then loads the Street Fighter game.\n",
    "        \n",
    "        super().__init__()\n",
    "        # specify action space and observation space\n",
    "        self.observation_space = Box(low = 0, high = 255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "        self.action_space = MultiBinary(12)\n",
    "        # startup an instance of the game\n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis',\n",
    "                               use_restricted_actions=retro.Actions.FILTERED)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    def reset(self):\n",
    "        \n",
    "        # Whenever you want to start a new game, you press the reset button.\n",
    "        # The game screen is made simple (black and white, smaller size) for the AI.\n",
    "        # The scoreboard is set to zero.\n",
    "        \n",
    "        \n",
    "        \n",
    "        obs = self.game.reset()\n",
    "        # current frame - previous frame = delta\n",
    "        \n",
    "        obs = self.preprocess(obs)\n",
    "        \n",
    "        self.previous_frame = obs\n",
    "        \n",
    "        #create a attribute to hold the score delta\n",
    "        self.score = 0\n",
    "        \n",
    "        return obs\n",
    "     \n",
    "        \n",
    "        \n",
    "        \n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    def preprocess(self, observation):\n",
    "        \n",
    "\n",
    "        # grayscale. remember by grayscaling we reduce the data to a single channel\n",
    "        # this simplifies image data, and reduces computational burden\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # RESIZE. smaller dimension means less pixels and faster processing. \n",
    "        resize = cv2.resize(gray, (84,84), interpolation = cv2.INTER_CUBIC)\n",
    "        \n",
    "        # add the channels value\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "        return channels\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------   \n",
    "    def step(self, action):\n",
    "        \n",
    "        # Here's the AI's turn to play!\n",
    "        # The AI makes a move (action).\n",
    "        # The game responds and shows a new screen.\n",
    "        # Again, the screen is made simple for the AI.\n",
    "        # The game checks what's changed on the screen from the last move.\n",
    "        # The game also checks if the AI's move improved the score.\n",
    "        # The AI gets feedback: how the game changed, how good the move was,\n",
    "        # and if the game is over.\n",
    "        \n",
    "        \n",
    "        \n",
    "       # how does this work?\n",
    "        # 1. we get the current frame\n",
    "        # 2. preprocess 200x256x3 -> 84x84x1\n",
    "        # 3. change in pixels current_frame - the last frame\n",
    "        \n",
    "        # Think of it like this: After every move the agent makes, the step \n",
    "        # method provides feedback about what changed and how good that move was.\n",
    "        \n",
    "        # take a step\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        \n",
    "        obs = self.preprocess(obs)\n",
    "        \n",
    "        # frame delta \n",
    "        frame_delta = obs-self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "        \n",
    "        # reshape the reward function\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "        \n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------   \n",
    "    def render(self, *args, **kwargs):\n",
    "        # You can watch the AI play the game. This is like the game being displayed on your TV.\n",
    "\n",
    "        self.game.render()\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "    def close(self):\n",
    "    # After the AI's done playing, you can shut down the game.\n",
    "\n",
    "        self.game.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac671b42-da47-45f9-b136-62950895d234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test out your new streetfighter class that will intiliaze your custom environment\n",
    "\n",
    "env = StreetFighter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f9e89-8e16-4c81-a544-6a3aec7f7ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9974de5f-6dc1-484f-b856-39bf6eb8d9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#using the old loop to run the environment\n",
    "\n",
    "obs = env.reset() # resets game to starting state\n",
    "done = False # tells if game is dead\n",
    "\n",
    "for game in range(1): # loops through game one time\n",
    "    while not done: # keep looping until dead\n",
    "        if done: # if dead, start(reset) again. \n",
    "            obs = env.reset()\n",
    "        env.render() # renders environement\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "        time.sleep(0.01) # to see some of the actions better\n",
    "        \n",
    "        if reward > 0: # print only when ryu hits. Possible todo: should blocking be rewarded?\n",
    "            print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c78486f-7012-49f2-bdb9-e0c19b9f28f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655244e3-6651-4eb5-98ba-82ad0cf30071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the following will show the frames being processed in gray scale\n",
    "# with the reduced size. \n",
    "\n",
    "obs, reward, done, info = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c08d3-ec01-4627-9087-4711e6bc0376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(obs, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439da8f3-ffdd-4b8f-a011-ad8d677705e1",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "PPO Hyperparameters to tune\n",
    "\n",
    "n_steps: batch size (frames in buffer)\n",
    "gamma: discount rate for calculating returns\n",
    "learning_rate: learning coefficient for optimizer\n",
    "clip_range: clipping amount for advantage calc\n",
    "gae_lambda: advantage smoothing parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a44ed53-b06b-42fc-93b2-9027058d9d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# need to install pytorch, optuna\n",
    "!pip3 install torch torchvision torchaudio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ffb65-9462-4b64-a570-d30a9ffdc8d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#install stable baseline \n",
    "!pip3 install \"stable-baselines3[extra] == 1.4.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb35823-6ef6-48c4-8e2b-c93878257181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# installing optuna\n",
    "!pip3 install optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e0ff5-6c15-416e-8a3e-5e834156f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies. \n",
    "\n",
    "\n",
    "# the optimization frame work - HPO\n",
    "import optuna \n",
    "\n",
    "#PPO algo for RL\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# bringin in the eval policy method for metric calculation\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Import the sb3 monitor for logging\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# import the vec wrappers to vectorize and frame stack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfad8a6-405c-4e07-9555-6730de44089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/' # saves logs here\n",
    "OPT_DIR = './opt/' # saves each model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01fe440-325c-42ab-8353-10b43a52ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return test hyperparameters -define the object function\n",
    "\n",
    "# Hyperparameters:\n",
    "\n",
    "# 'n_steps': The number of steps to run for each environment per update.\n",
    "# Optuna will try different values between 2048 and 8192.\n",
    "\n",
    "# 'gamma': Discount factor. It determines how much importance to give to future rewards.\n",
    "# Optuna will try different values between 0.8 and 0.9999 in a logarithmic manner.\n",
    "\n",
    "# 'learning_rate': The rate at which the algorithm learns.\n",
    "# Optuna will try different logarithmic values between 1e-5 and 1e-4.\n",
    "\n",
    "# 'clip_range': Used in the PPO algorithm to clip the new policy probability to prevent large updates.\n",
    "# Optuna will try different values between 0.1 and 0.4.\n",
    "\n",
    "# 'gae_lambda': Lambda value used in Generalized Advantage Estimation (a technique in reinforcement learning).\n",
    "# Optuna will try different values between 0.8 and 0.99.\n",
    "\n",
    "\n",
    "\n",
    "def optimize_ppo(trial):\n",
    "    return {\n",
    "        'n_steps': trial.suggest_int('n_steps', 2048, 8192),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 0.8, 0.9999),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
    "        'clip_range': trial.suggest_uniform('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda': trial.suggest_uniform('gae_lambda', 0.8, 0.99)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ae23f-7ddd-4c01-ad8a-e1e3fad5311f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805eb759-c273-4375-b34e-244073fef49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a training loop and return mean reward\n",
    "# Run a training loop and return mean reward \n",
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        model_params = optimize_ppo(trial) \n",
    "\n",
    "        # Create environment \n",
    "        env = StreetFighter()\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "        # Create algo \n",
    "        model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        model.learn(total_timesteps=30000)\n",
    "        #model.learn(total_timesteps=100000)\n",
    "\n",
    "        # Evaluate model \n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "        env.close()\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "    except Exception as e:\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592796c-2fb9-46d8-95fe-083faaaf7775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the experiment \n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent, n_trials=100, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2857269-bc2d-4c5f-b2de-9912806bd37d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb736910-9890-47d3-8e39-4a580f216ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PPO.load(os.path.join(OPT_DIR, 'best_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a475127-e723-4856-9484-c51ed7d5cbfc",
   "metadata": {},
   "source": [
    "## Setup Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce98d1cd-dd10-4ee5-8888-820dabe46a40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import base callback\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cff8eb-6d7f-4746-8451-164200d8cfc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58cc4e5-842e-43f7-b64f-22e5079001b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b855900b-aa79-408f-a99b-6cf9c940c7be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c20bc-68e2-42e6-8d7c-ef8c7aed8649",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0159ba-27bc-4d5f-b9b8-ffe911f83d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create environment \n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9454b0-3f2c-4b2d-a297-e7dae2de6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = study.best_params\n",
    "model_params['n_steps'] = 7488  # set n_steps to 7488 or a factor of 64\n",
    "# model_params['learning_rate'] = 5e-7\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b0cb67-e64a-49e6-97b0-d31db9024d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc0c89c-22da-4e16-a349-b95d3bd3edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload previous weights from HPO\n",
    "model.load(os.path.join(OPT_DIR, 'trial_5_best_model.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf85d80-782d-4b1f-8ab8-7d0f52797703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick off training \n",
    "model.learn(total_timesteps=100000, callback=callback)\n",
    "# model.learn(total_timestep=5000000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290acc27-1fef-49f8-ab92-896269660fac",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb191f-0897-404e-851d-63c7f125b1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PPO.load('./train/best_model_5460000.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7e8c4-dcba-4ece-a7c2-2d5182116a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, render=True, n_eval_episodes=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f223c6-1a33-472d-8daf-45b63debb63f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ceea3-c921-41b8-b27a-4ff21b60ec6a",
   "metadata": {},
   "source": [
    "## Test out the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c69b548-34f0-44a6-8054-6009885b3e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db0316-352b-4b96-af36-86b8c9c764e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03494e-5d2f-455d-b7c7-1b9aea7f6295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.step(model.predict(obs)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae91ab-f539-47fd-83a8-68bd48ec6409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "# Set flag to flase\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.001)\n",
    "        print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76323c-f6ee-4b3d-bddb-b4e53ac55894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
